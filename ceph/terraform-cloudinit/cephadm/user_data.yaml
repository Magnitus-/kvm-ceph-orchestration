#cloud-config
merge_how:
 - name: list
   settings: [append, no_replace]
 - name: dict
   settings: [no_replace, recurse_list]

write_files:
  - path: /opt/ceph/tls/ca.crt
    owner: root:root
    permissions: "0400"
    content: |
      ${indent(6, tls.ca_cert)}
  - path: /opt/ceph/tls/server.crt
    owner: root:root
    permissions: "0400"
    content: |
      ${indent(6, tls.server_cert)}
  - path: /opt/ceph/tls/server.key
    owner: root:root
    permissions: "0400"
    content: |
      ${indent(6, tls.server_key)}
  - path: /opt/ceph/ssh/ssh_key.pub
    owner: root:root
    permissions: "0400"
    content: |
      ${indent(6, ssh.public_key)}
  - path: /opt/ceph/ssh/ssh_key
    owner: root:root
    permissions: "0400"
    content: |
      ${indent(6, ssh.private_key)}
  - path: /opt/ceph/conf/services.yml
    owner: root:root
    permissions: "0500"
    content: |
      ---
      service_type: mon
      service_name: mon
      placement:
        count: 3
        label: "mon"
      ---
      service_type: mgr
      service_name: mgr
      networks:
        - ${public_network}
      placement:
        count: 2
        label: "mgr"
  - path: /opt/ceph/conf/rgw_services.yml
    owner: root:root
    permissions: "0500"
    content: |
      ---
      service_type: rgw
      service_id: ${rgw_zone}
      placement:
        label: rgw
        count_per_host: 1
      spec:
        rgw_frontend_type: "beast"
        rgw_frontend_port: 8080
      ---
      service_type: ingress
      service_id: rgw.${rgw_zone}
      placement:
        label: rgw
        count: 3
      backend_service: rgw.${rgw_zone}
      virtual_ip: ${rgw_ingress_ip}
      frontend_port: 4443
      monitor_port: 1967
      virtual_interface_networks:
        - ${public_network}
      ssl_cert: |
        ${indent(8, tls.server_cert)}
        ${indent(8, tls.server_key)}
  - path: /opt/ceph/conf/hosts.yml
    owner: root:root
    permissions: "0500"
    content: |
%{ for node in ceph_cluster ~}
      ---
      service_type: host
      hostname: ${node.domain}
      addr: ${node.ip}
      labels:
%{ if node.monitor ~}
        - mon
%{ endif ~}
%{ if node.manager ~}
        - mgr
%{ endif ~}
%{ if node.osd ~}
        - osd
%{ endif ~}
%{ if node.rgw ~}
        - rgw
%{ endif ~}
%{ if node.admin ~}
        - _admin
%{ endif ~}
%{ endfor ~}
  - path: /opt/ceph/scripts/bootstrap.sh
    owner: root:root
    permissions: "0500"
    content: |
      cephadm bootstrap --mon-ip ${self_ip} \
                        --allow-fqdn-hostname \
                        --ssh-user ubuntu \
                        --dashboard-key /opt/ceph/tls/server.key \
                        --dashboard-crt /opt/ceph/tls/server.crt \
                        --ssh-private-key /opt/ceph/ssh/ssh_key \
                        --ssh-public-key /opt/ceph/ssh/ssh_key.pub
      ceph orch host add $(hostname) --labels=mon
      ceph orch host add $(hostname) --labels=mgr
      ceph orch apply -i /opt/ceph/conf/services.yml
  - path: /opt/ceph/scripts/expand_cluster.sh
    owner: root:root
    permissions: "0500"
    content: |
      ceph orch apply -i /opt/ceph/conf/hosts.yml
  - path: /opt/ceph/scripts/deploy_osds.sh
    owner: root:root
    permissions: "0500"
    content: |
      ceph orch apply osd --all-available-devices
  - path: /opt/ceph/scripts/deploy_rgw.sh
    owner: root:root
    permissions: "0500"
    content: |
      ceph osd erasure-code-profile set k2m1 k=2 m=1 crush-failure-domain=host
      #Control pool
      ceph osd pool create ${rgw_zone}.rgw.control 32 32 replicated replicated_rule
      ceph osd pool set ${rgw_zone}.rgw.control size 3
      ceph osd pool set ${rgw_zone}.rgw.control min_size 2
      ceph osd pool application enable ${rgw_zone}.rgw.control rgw
      #Meta pool
      ceph osd pool create ${rgw_zone}.rgw.meta 32 32 replicated replicated_rule
      ceph osd pool set ${rgw_zone}.rgw.meta size 3
      ceph osd pool set ${rgw_zone}.rgw.meta min_size 2
      ceph osd pool application enable ${rgw_zone}.rgw.meta rgw
      #Log pool
      ceph osd pool create ${rgw_zone}.rgw.log 32 32 replicated replicated_rule
      ceph osd pool set ${rgw_zone}.rgw.log size 3
      ceph osd pool set ${rgw_zone}.rgw.log min_size 2
      ceph osd pool application enable ${rgw_zone}.rgw.log rgw
      #Otp pool
      ceph osd pool create ${rgw_zone}.rgw.otp 32 32 replicated replicated_rule
      ceph osd pool set ${rgw_zone}.rgw.otp size 3
      ceph osd pool set ${rgw_zone}.rgw.otp min_size 2
      ceph osd pool application enable ${rgw_zone}.rgw.otp rgw
      #Default placement group index pool
      ceph osd pool create ${rgw_zone}.rgw.buckets.index 64 64 replicated replicated_rule
      ceph osd pool set ${rgw_zone}.rgw.buckets.index size 3
      ceph osd pool set ${rgw_zone}.rgw.buckets.index min_size 2
      ceph osd pool application enable ${rgw_zone}.rgw.buckets.index rgw
      #Default placement group extra data pool
      ceph osd pool create ${rgw_zone}.rgw.buckets.non-ec 64 64 replicated replicated_rule
      ceph osd pool set ${rgw_zone}.rgw.buckets.non-ec size 3
      ceph osd pool set ${rgw_zone}.rgw.buckets.non-ec min_size 2
      ceph osd pool application enable ${rgw_zone}.rgw.buckets.non-ec rgw
      #Default placement group standard storage class
      ceph osd pool create ${rgw_zone}.rgw.buckets.data 128 128 replicated replicated_rule
      ceph osd pool set ${rgw_zone}.rgw.buckets.data size 3
      ceph osd pool set ${rgw_zone}.rgw.buckets.data min_size 2
      ceph osd pool application enable ${rgw_zone}.rgw.buckets.data rgw
      #Default placement group compact storage class
      ceph osd pool create ${rgw_zone}.compact.rgw.buckets.data 128 128 erasure k2m1
      ceph osd pool set ${rgw_zone}.compact.rgw.buckets.data min_size 3
      ceph osd pool application enable ${rgw_zone}.compact.rgw.buckets.data rgw
      ceph orch apply -i /opt/ceph/conf/rgw_services.yml
      sleep 30
      #Deploy gateways
      radosgw-admin realm create --rgw-realm=${rgw_zone} --default
      #Create zone
      radosgw-admin zonegroup create --rgw-zonegroup=${rgw_zone} --rgw-realm=${rgw_zone} --master --default
      radosgw-admin zone create --rgw-zonegroup=${rgw_zone} --rgw-zone=${rgw_zone} --master --default
      radosgw-admin zonegroup placement add --rgw-zonegroup ${rgw_zone} --placement-id default-placement --storage-class COMPACT
      radosgw-admin zone placement add --rgw-zone ${rgw_zone} --placement-id default-placement --storage-class COMPACT --data-pool ${rgw_zone}.compact.rgw.buckets.data --compression lz4
      #Cleanup default zone
      radosgw-admin zonegroup delete --rgw-zonegroup=default --rgw-zone=default
      radosgw-admin period update --commit
      radosgw-admin zone delete --rgw-zone=default
      radosgw-admin period update --commit
      ceph tell mon.* injectargs --mon_allow_pool_delete true
      ceph osd pool rm default.rgw.log default.rgw.log --yes-i-really-really-mean-it
      ceph osd pool rm default.rgw.control default.rgw.control --yes-i-really-really-mean-it
      ceph osd pool rm default.rgw.meta default.rgw.meta --yes-i-really-really-mean-it
      ceph tell mon.* injectargs --mon_allow_pool_delete false
  - path: /opt/ceph/scripts/generate_rgw_users.sh
    owner: root:root
    permissions: "0500"
    content: |
%{ for user in rgw_users ~}
      radosgw-admin user create --tenant=${user.tenant} --uid=${user.uid} --display-name="${user.display_name}" --email=${user.email}
      radosgw-admin subuser create --tenant=${user.tenant} --uid=${user.uid} --subuser=${user.uid}:swift --access=${user.swift_access}
      radosgw-admin key create --tenant=${user.tenant} --uid=${user.uid} --key-type=s3 --access-key=${user.uid} --secret-key=${user.secret_key}
      radosgw-admin key create --tenant=${user.tenant} --uid=${user.uid} --subuser=${user.uid}:swift --key-type=swift --secret-key=${user.secret_key}
%{ endfor ~}

packages:
  - jq

runcmd:
  - wget https://download.ceph.com/rpm-18.2.0/el9/noarch/cephadm
  - chmod +x cephadm
  - ./cephadm add-repo --release reef
  - ./cephadm install
  - rm cephadm
  - cephadm install ceph-common
